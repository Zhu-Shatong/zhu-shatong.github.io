<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Shatong Zhu </title> <meta name="author" content="Shatong Zhu"> <meta name="description" content="The personal website of Shatong Zhu. "> <meta name="keywords" content="Shatong Zhu, Zhu Shatong, Êú±Ê≤ôÊ°ê, ÂêåÊµé, Tongji, machine learning, AI, artificial intelligence, RL"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?bebcb3db29cbba865b57985bedc0bb04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Titillium%20Web:400,500,700"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/tongji.ico?f07f90d72712d83c3aca4d6d6c85d638"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhushatong.com/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <article> <div class="row"> <div class="col-3 col-md-3"> <img src="assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" alt="prof_pic.jpg"> </div> <div class="col-9 col-md-9"> <h1 class="post-title"> Shatong Zhu „ÄåÊú±Ê≤ôÊ°ê„Äç </h1> <p class="desc">Research Assistant at the <a href="http://cs1.tongji.edu.cn/~junqiao/" rel="external nofollow noopener" target="_blank">Tongji TiEV Lab</a><br><br> Undergraduate B.Eng Student, <br> Data Science and Big Data Technology, <br> <a href="https://en.tongji.edu.cn/p/#/" rel="external nofollow noopener" target="_blank">Tongji University</a>, Shanghai, China. </p> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%68%61%74%6F%6E%67.%7A%68%75@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/Zhu-Shatong" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/shatongzhu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=bfDoxPAAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> </div> </div> </div> <div class="clearfix"> <p>I am a fourth-year B.Eng student in Data Science at the School of Computer Science, <a href="https://en.tongji.edu.cn/p/#/" rel="external nofollow noopener" target="_blank">Tongji University</a>, adviced by <a href="http://cs1.tongji.edu.cn/~junqiao/" rel="external nofollow noopener" target="_blank">Junqiao Zhao</a>. I have been honored with National Scholarship twice.</p> <p><strong>Research Interests:</strong> I have a broad interest in advancing the next generation of artificial intelligence, with a primary focus on reinforcement learning (RL) and robotic manipulation. Currently, I am a Research Assistant at the Tongji TiEV Lab, where I explore the generalization capabilities of robotic manipulation through offline meta-reinforcement learning.</p> <p><strong>Data Science:</strong> As a Data Science student, I am passionate about applying mathematical techniques to solve real-world challenges, particularly in areas such as data analysis, data mining, and operations research.</p> <p><strong>Previously:</strong> I have interned as an algorithm engineer at <a href="https://en.meetsocial.com/" rel="external nofollow noopener" target="_blank">Meetsocial</a> and <a href="https://www.zhongan.com/" rel="external nofollow noopener" target="_blank">ZhongAn Insurance</a>. Additionally, I have contributed to research on Multi-Agent Reinforcement Learning (MARL) as a Research Assistant at the Tongji TiEV Lab, and worked on fine-tuning Large Language Models (LLM) at the Tongji FinLab.</p> <p>Email: shatong.zhu[AT]gmail.com</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="color: black; width: 20%">Nov 03, 2024</th> <td style="color: black;"> üèÜ I was awarded the National Scholarship for the 2024 academic year! (Top 0.2% nationwide) </td> </tr> <tr> <th scope="row" style="color: black; width: 20%">Sep 26, 2024</th> <td style="color: black;"> üìÑ Our paper UNICORN is accepted at NeurIPS 2024 as a spotlight! </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications <a href="publications/">[full list]</a> </h2> (*) denotes equal contribution <br> <br> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr" style="min-width: 250px;"> <abbr class="badge" style="min-width: 220px;">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/powqmix-480.webp 480w,/assets/img/publication_preview/powqmix-800.webp 800w,/assets/img/publication_preview/powqmix-1400.webp 1400w," type="image/webp" sizes="220px"> <img src="/assets/img/publication_preview/powqmix.png" class="preview z-depth-1 rounded" width="100%" height="auto" style=" min-width: 220px; " alt="powqmix.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="huang2024powqimix" class="col-sm-8"> <div class="title">POWQMIX: Weighted Value Factorization with Potentially Optimal Joint Actions Recognition for Cooperative Multi-Agent Reinforcement Learning</div> <div class="author"> Chang Huang*,¬†Shatong Zhu*,¬†Junqiao Zhao,¬†Hongtu Zhou,¬†Chen Ye,¬†Tiantian Feng,¬†and Changjun Jiang </div> <div class="periodical"> <em>In arXiv preprint arXiv:2405.08036. Under review</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2405.08036" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Value function factorization methods are commonly used in cooperative multi-agent reinforcement learning, with QMIX receiving significant attention. Many QMIX-based methods introduce monotonicity constraints between the joint action value and individual action values to achieve decentralized execution. However, such constraints limit the representation capacity of value factorization, restricting the joint action values it can represent and hindering the learning of the optimal policy. To address this challenge, we propose the Potentially Optimal Joint Actions Weighted QMIX (POWQMIX) algorithm, which recognizes the potentially optimal joint actions and assigns higher weights to the corresponding losses of these joint actions during training. We theoretically prove that with such a weighted training approach the optimal policy is guaranteed to be recovered. Experiments in matrix games, difficulty-enhanced predator-prey, and StarCraft II Multi-Agent Challenge environments demonstrate that our algorithm outperforms the state-of-the-art value-based multi-agent reinforcement learning methods. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr" style="min-width: 250px;"> <abbr class="badge" style="min-width: 220px;">NeurIPS</abbr> <span class="award badge" style="min-width: 220px;">Spotlight</span> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unicorn-480.webp 480w,/assets/img/publication_preview/unicorn-800.webp 800w,/assets/img/publication_preview/unicorn-1400.webp 1400w," type="image/webp" sizes="220px"> <img src="/assets/img/publication_preview/unicorn.png" class="preview z-depth-1 rounded" width="100%" height="auto" style=" min-width: 220px; " alt="unicorn.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="li2024towards" class="col-sm-8"> <div class="title">Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning</div> <div class="author"> Lanqing Li*,¬†Hai Zhang*,¬†Xinyu Zhang,¬†Shatong Zhu,¬†Yang YU,¬†Junqiao Zhao,¬†and Pheng-Ann Heng </div> <div class="periodical"> <em>In The 38th Annual Conference on Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <span class="honor"> Spotlight Presentation [Top 1%] </span> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=QFUsZvw9mx" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/betray12138/UNICORN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NeurIPS2024/unicorn_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Spotlight</p> </div> <div class="abstract hidden"> <p> As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable M and its latent representation Z by implementing various approximate bounds. Such theoretical insight offers ample design freedom for novel algorithms. As demonstrations, we propose a supervised and a self-supervised implementation of I(Z;M), and empirically show that the corresponding optimization algorithms exhibit remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures. This work lays the information theoretic foundation for COMRL methods, leading to a better understanding of task representation learning in the context of reinforcement learning. </p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> ¬© Copyright 2025 Shatong Zhu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>